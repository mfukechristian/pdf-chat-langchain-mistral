{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08370560",
   "metadata": {},
   "source": [
    "# Learn Retrieval-Augmented Generation (RAG) by Building a PDF Chatbot – A Beginner’s Guide with Mistral AI and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb358b",
   "metadata": {},
   "source": [
    "### What is Retrieval-Augmented Generation (RAG)?\n",
    "Retrieval-Augmented Generation (RAG) is a technique used in AI that combines two things: a retrieval system and a language model. Instead of relying only on what the AI was trained on, RAG lets the AI look up relevant information from an external source (like a PDF) before generating a response. First, it retrieves the most relevant text snippets based on the input question, then it uses those snippets to help generate a more accurate and informed answer. This makes RAG especially useful for answering questions about recent or specific topics. It's like giving the AI a quick Google search to help it respond better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55b010",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Architecture\n",
    "\n",
    "![RAG Architecture](images/rag_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4625f16",
   "metadata": {},
   "source": [
    "**Step 1: Data Collection**  \n",
    "Gather your source documents (PDFs, Word files, websites, etc.) that contain the information you want the AI to use.\n",
    "\n",
    "**Step 2: Chunking (Split the Data)**  \n",
    "Break down large texts into smaller, manageable pieces called \"chunks\" (e.g., a few paragraphs per chunk).\n",
    "\n",
    "**Step 3: Embedding (Vectorization)**  \n",
    "Convert each chunk into a vector (a numerical representation of the text’s meaning) using an embedding model.\n",
    "\n",
    "**Step 4: Store in Vector Database**  \n",
    "Save the vectors in a vector database like FAISS, Pinecone, or Weaviate to allow fast and meaningful searches.\n",
    "\n",
    "**Step 5: Retrieval**  \n",
    "When a user asks a question, convert the question into a vector and search the database to retrieve the most relevant chunks.\n",
    "\n",
    "**Step 6: Generation**  \n",
    "Send both the original question and the retrieved chunks to a language model (like GPT or Mistral) to generate a response that is informed and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a7a1f",
   "metadata": {},
   "source": [
    "### Let Building a PDF Chatbot to understand Retrieval-Augmented Generation (RAG) with Mistral AI and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161353df-0b8e-4d3b-aa5f-ea0eddb155d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain            \n",
    "%pip install langchain_mistralai\n",
    "%pip install python-dotenv        \n",
    "%pip install langchain_community \n",
    "%pip install pypdf              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae2e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv   # Import the function to load environment variables\n",
    "import os                        # Import the os module to access environment variables\n",
    "\n",
    "load_dotenv()                    # Load the variables from the .env file\n",
    "api_key = os.getenv(\"MISTRALAI_API_KEY\")  # Get your Mistral API key from the https://admin.mistral.ai/organization/api-keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65aff843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1d861",
   "metadata": {},
   "source": [
    "This code loads below a PDF file and splits it into pages. PyPDFLoader reads the file, and .load() returns each page as a chunk. print(len(...)) shows how many pages were loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4055f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "loader_pdf = PyPDFLoader(\"Introduction_to_Data_and_Data_Science.pdf\")\n",
    "pages_pdf = loader_pdf.load()\n",
    "print(len(pages_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca3160",
   "metadata": {},
   "source": [
    "This code splits the loaded PDF pages into smaller chunks. It uses a period (\".\") to split text, creates chunks of 500 characters, and overlaps 50 characters to keep contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d572e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "char_splitter = CharacterTextSplitter(separator = \".\", \n",
    "                                      chunk_size = 500, \n",
    "                                      chunk_overlap = 50)\n",
    "pages_char_split = char_splitter.split_documents(pages_pdf)\n",
    "print(len(pages_char_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c4e6a",
   "metadata": {},
   "source": [
    "This code creates embeddings (number-based text representations) using Mistral’s embedding model and your API key. Then, it converts the split text chunks into vectors and stores them in an in-memory vector database, allowing fast searching of similar text during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370297b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfuke\\OneDrive\\Bureau\\projects\\pdf-chat-langchain-mistral\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\mfuke\\OneDrive\\Bureau\\projects\\pdf-chat-langchain-mistral\\venv\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings=MistralAIEmbeddings( model=\"mistral-embed\",api_key=api_key)\n",
    "vector_store = InMemoryVectorStore.from_documents(documents= pages_char_split, \n",
    "                                                  embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3550d0",
   "metadata": {},
   "source": [
    "retriever = vector_store.as_retriever() creates a search tool from the vector store that finds the most relevant text chunks based on user input, helping the AI provide accurate, informed answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6dc336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029ccbf",
   "metadata": {},
   "source": [
    "his prompt instructs the AI to use only the retrieved context (from the vector store) when answering the question, making sure the answer is based on the specific info found—not just guessing. It also asks the AI to mention which lecture the info came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cc1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "Answer the following question:\n",
    "{question}\n",
    "\n",
    "To answer the question, use only the following context:\n",
    "{context}\n",
    "\n",
    "At the end of the response, specify the name of the lecture this context is taken from in the format:\n",
    "Resources: *Lecture Title*\n",
    "where *Lecture Title* should be substituted with the title of all resource lectures.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f27acf",
   "metadata": {},
   "source": [
    "This code initializes the Mistral chat model using your API key and selects the \"mistral-large-latest\" version, so you can start generating AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d043277",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model = ChatMistralAI(api_key=api_key, model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73a3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me a summary of the lecture and the resources used in it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76322d1a",
   "metadata": {},
   "source": [
    "This code creates a chain that retrieves relevant context, formats a prompt with the user’s question, sends it to the AI model for an answer, and returns the final response as a clean text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e98ec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lecture provides an introduction to data and data science, focusing on the concepts of analysis and analytics. It explains that analysis involves breaking down large datasets into smaller, manageable chunks to study them individually and understand their relationships. The lecture emphasizes that analysis is retrospective, focusing on past events to explain how or why something happened. It also discusses the applicability of various programming and software tools in data science, noting that learning software tools can be easier than learning programming languages, especially with relevant business and theoretical knowledge. However, strong theoretical preparation might make software tools feel restrictive.\n",
      "\n",
      "Resources: Introduction to Data and Data Science\n"
     ]
    }
   ],
   "source": [
    "# Create the chain that links retriever, prompt, chat model, and output parser\n",
    "chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}  # Get context + pass question\n",
    "    | prompt_template                                         # Format prompt with template\n",
    "    | chat                                                   # Generate answer from chat model\n",
    "    | StrOutputParser()                                      # Parse output as string\n",
    ")\n",
    "\n",
    "# Use the chain to answer a question\n",
    "response = chain.invoke(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25514a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (5.34.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.2.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.115.14)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.3 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (1.10.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.33.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (2.3.0)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.12.1)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (4.14.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio) (0.34.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio-client==1.10.3->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from gradio-client==1.10.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mfuke\\onedrive\\bureau\\projects\\pdf-chat-langchain-mistral\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9771c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "\n",
    "# Assume api_key is set in an upper cell and available here\n",
    "\n",
    "vector_store = None\n",
    "retriever = None\n",
    "chain = None\n",
    "chat = ChatMistralAI(api_key=api_key, model=\"mistral-large-latest\")\n",
    "\n",
    "TEMPLATE = '''\n",
    "Answer the following question:\n",
    "{question}\n",
    "\n",
    "To answer the question, use only the following context:\n",
    "{context}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "def embed_pdf(file):\n",
    "    global vector_store, retriever, chain\n",
    "    \n",
    "    loader_pdf = PyPDFLoader(file.name)\n",
    "    pages_pdf = loader_pdf.load()\n",
    "\n",
    "    char_splitter = CharacterTextSplitter(separator=\".\", chunk_size=500, chunk_overlap=50)\n",
    "    pages_char_split = char_splitter.split_documents(pages_pdf)\n",
    "\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\", api_key=api_key)\n",
    "    vector_store = InMemoryVectorStore.from_documents(documents=pages_char_split, embedding=embeddings)\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    chain = (\n",
    "        {'context': retriever, 'question': RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | chat\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return \"Embedding complete! Now you can ask questions.\"\n",
    "\n",
    "def ask_question(question):\n",
    "    global chain\n",
    "    if not chain:\n",
    "        return \"Please upload and embed a PDF first!\"\n",
    "    response = chain.invoke(question)\n",
    "    return response\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# PDF Chat with Retrieval-Augmented Generation (RAG)\")\n",
    "    pdf_input = gr.File(label=\"Upload a PDF\")\n",
    "    embed_btn = gr.Button(\"Embed PDF\")\n",
    "    output_embed = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    question_input = gr.Textbox(label=\"Ask a question about the PDF\")\n",
    "    ask_btn = gr.Button(\"Ask\")\n",
    "    output_answer = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    embed_btn.click(embed_pdf, inputs=pdf_input, outputs=output_embed)\n",
    "    ask_btn.click(ask_question, inputs=question_input, outputs=output_answer)\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
