{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08370560",
   "metadata": {},
   "source": [
    "# Learn Retrieval-Augmented Generation (RAG) by Building a PDF Chatbot – A Beginner’s Guide with Mistral AI and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb358b",
   "metadata": {},
   "source": [
    "### What is Retrieval-Augmented Generation (RAG)?\n",
    "Retrieval-Augmented Generation (RAG) is a technique used in AI that combines two things: a retrieval system and a language model. Instead of relying only on what the AI was trained on, RAG lets the AI look up relevant information from an external source (like a PDF) before generating a response. First, it retrieves the most relevant text snippets based on the input question, then it uses those snippets to help generate a more accurate and informed answer. This makes RAG especially useful for answering questions about recent or specific topics. It's like giving the AI a quick Google search to help it respond better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55b010",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Architecture\n",
    "\n",
    "![RAG Architecture](images/rag_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4625f16",
   "metadata": {},
   "source": [
    "**Step 1: Data Collection**  \n",
    "Gather your source documents (PDFs, Word files, websites, etc.) that contain the information you want the AI to use.\n",
    "\n",
    "**Step 2: Chunking (Split the Data)**  \n",
    "Break down large texts into smaller, manageable pieces called \"chunks\" (e.g., a few paragraphs per chunk).\n",
    "\n",
    "**Step 3: Embedding (Vectorization)**  \n",
    "Convert each chunk into a vector (a numerical representation of the text’s meaning) using an embedding model.\n",
    "\n",
    "**Step 4: Store in Vector Database**  \n",
    "Save the vectors in a vector database like FAISS, Pinecone, or Weaviate to allow fast and meaningful searches.\n",
    "\n",
    "**Step 5: Retrieval**  \n",
    "When a user asks a question, convert the question into a vector and search the database to retrieve the most relevant chunks.\n",
    "\n",
    "**Step 6: Generation**  \n",
    "Send both the original question and the retrieved chunks to a language model (like GPT or Mistral) to generate a response that is informed and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a7a1f",
   "metadata": {},
   "source": [
    "### Let Building a PDF Chatbot to understand Retrieval-Augmented Generation (RAG) with Mistral AI and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161353df-0b8e-4d3b-aa5f-ea0eddb155d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain            # Core LangChain library for building LLM apps\n",
    "%pip install langchain_mistralai # Mistral AI LangChain package\n",
    "%pip install python-dotenv        # Load secret keys (like API keys) from a .env file\n",
    "%pip install langchain_community  # Extra tools and integrations from the LangChain community\n",
    "%pip install pypdf                # Read and extract text from PDF files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv   # Import the function to load environment variables\n",
    "import os                        # Import the os module to access environment variables\n",
    "\n",
    "load_dotenv()                    # Load the variables from the .env file\n",
    "api_key = os.getenv(\"MISTRALAI_API_KEY\")  # Get your Mistral API key from the https://admin.mistral.ai/organization/api-keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aff843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1d861",
   "metadata": {},
   "source": [
    "This code loads below a PDF file and splits it into pages. PyPDFLoader reads the file, and .load() returns each page as a chunk. print(len(...)) shows how many pages were loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_pdf = PyPDFLoader(\"Introduction_to_Data_and_Data_Science.pdf\")\n",
    "pages_pdf = loader_pdf.load()\n",
    "print(len(pages_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca3160",
   "metadata": {},
   "source": [
    "This code splits the loaded PDF pages into smaller chunks. It uses a period (\".\") to split text, creates chunks of 500 characters, and overlaps 50 characters to keep contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d572e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = CharacterTextSplitter(separator = \".\", \n",
    "                                      chunk_size = 500, \n",
    "                                      chunk_overlap = 50)\n",
    "pages_char_split = char_splitter.split_documents(pages_pdf)\n",
    "print(len(pages_char_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c4e6a",
   "metadata": {},
   "source": [
    "This code creates embeddings (number-based text representations) using Mistral’s embedding model and your API key. Then, it converts the split text chunks into vectors and stores them in an in-memory vector database, allowing fast searching of similar text during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370297b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=MistralAIEmbeddings( model=\"mistral-embed\",api_key=api_key)\n",
    "vector_store = InMemoryVectorStore.from_documents(documents= pages_char_split, \n",
    "                                                  embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3550d0",
   "metadata": {},
   "source": [
    "retriever = vector_store.as_retriever() creates a search tool from the vector store that finds the most relevant text chunks based on user input, helping the AI provide accurate, informed answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029ccbf",
   "metadata": {},
   "source": [
    "his prompt instructs the AI to use only the retrieved context (from the vector store) when answering the question, making sure the answer is based on the specific info found—not just guessing. It also asks the AI to mention which lecture the info came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "Answer the following question:\n",
    "{question}\n",
    "\n",
    "To answer the question, use only the following context:\n",
    "{context}\n",
    "\n",
    "At the end of the response, specify the name of the lecture this context is taken from in the format:\n",
    "Resources: *Lecture Title*\n",
    "where *Lecture Title* should be substituted with the title of all resource lectures.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f27acf",
   "metadata": {},
   "source": [
    "This code initializes the Mistral chat model using your API key and selects the \"mistral-large-latest\" version, so you can start generating AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d043277",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model = ChatMistralAI(api_key=api_key, model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me a summary of the lecture and the resources used in it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76322d1a",
   "metadata": {},
   "source": [
    "This code creates a chain that retrieves relevant context, formats a prompt with the user’s question, sends it to the AI model for an answer, and returns the final response as a clean text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain that links retriever, prompt, chat model, and output parser\n",
    "chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}  # Get context + pass question\n",
    "    | prompt_template                                         # Format prompt with template\n",
    "    | chat                                                   # Generate answer from chat model\n",
    "    | StrOutputParser()                                      # Parse output as string\n",
    ")\n",
    "\n",
    "# Use the chain to answer a question\n",
    "response = chain.invoke(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25514a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9771c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "\n",
    "# Assume api_key is set in an upper cell and available here\n",
    "\n",
    "vector_store = None\n",
    "retriever = None\n",
    "chain = None\n",
    "chat = ChatMistralAI(api_key=api_key, model=\"mistral-large-latest\")\n",
    "\n",
    "TEMPLATE = '''\n",
    "Answer the following question:\n",
    "{question}\n",
    "\n",
    "To answer the question, use only the following context:\n",
    "{context}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "def embed_pdf(file):\n",
    "    global vector_store, retriever, chain\n",
    "    \n",
    "    loader_pdf = PyPDFLoader(file.name)\n",
    "    pages_pdf = loader_pdf.load()\n",
    "\n",
    "    char_splitter = CharacterTextSplitter(separator=\".\", chunk_size=500, chunk_overlap=50)\n",
    "    pages_char_split = char_splitter.split_documents(pages_pdf)\n",
    "\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\", api_key=api_key)\n",
    "    vector_store = InMemoryVectorStore.from_documents(documents=pages_char_split, embedding=embeddings)\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    chain = (\n",
    "        {'context': retriever, 'question': RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | chat\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return \"Embedding complete! Now you can ask questions.\"\n",
    "\n",
    "def ask_question(question):\n",
    "    global chain\n",
    "    if not chain:\n",
    "        return \"Please upload and embed a PDF first!\"\n",
    "    response = chain.invoke(question)\n",
    "    return response\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# PDF Chat with Retrieval-Augmented Generation (RAG)\")\n",
    "    pdf_input = gr.File(label=\"Upload a PDF\")\n",
    "    embed_btn = gr.Button(\"Embed PDF\")\n",
    "    output_embed = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    question_input = gr.Textbox(label=\"Ask a question about the PDF\")\n",
    "    ask_btn = gr.Button(\"Ask\")\n",
    "    output_answer = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    embed_btn.click(embed_pdf, inputs=pdf_input, outputs=output_embed)\n",
    "    ask_btn.click(ask_question, inputs=question_input, outputs=output_answer)\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
